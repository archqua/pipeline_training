{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwVo1ZAC86ddvJRT96C60c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2288749c1538480ebd6d11b12e92deab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58b8b85dbbf94eff8f0df0d9f62f12dd",
              "IPY_MODEL_4478eb5891054e43b00e398d2089d2d2",
              "IPY_MODEL_9d3b2bfbd7cc43339f488162b2277b99"
            ],
            "layout": "IPY_MODEL_9f2dade40a88419aacc2b4eb28510f88"
          }
        },
        "58b8b85dbbf94eff8f0df0d9f62f12dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7784925b6d774ed7bf9623781bcc2395",
            "placeholder": "​",
            "style": "IPY_MODEL_4226d4c8d65041428d93c51507a55d62",
            "value": "100%"
          }
        },
        "4478eb5891054e43b00e398d2089d2d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c9404adcd084c9fa07422c1bcb294e8",
            "max": 469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_867c63bed6eb44ebabb61693f8fd2a04",
            "value": 469
          }
        },
        "9d3b2bfbd7cc43339f488162b2277b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc96e6ed623f44909647837c7ea25546",
            "placeholder": "​",
            "style": "IPY_MODEL_96ce55b0990e4ead9ffd154e2a331059",
            "value": " 469/469 [01:22&lt;00:00,  5.80it/s]"
          }
        },
        "9f2dade40a88419aacc2b4eb28510f88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7784925b6d774ed7bf9623781bcc2395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4226d4c8d65041428d93c51507a55d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c9404adcd084c9fa07422c1bcb294e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867c63bed6eb44ebabb61693f8fd2a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc96e6ed623f44909647837c7ea25546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96ce55b0990e4ead9ffd154e2a331059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archqua/pipeline_training/blob/master/mnist_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZMhanU3jiaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Step Classification Pipeline with Human Feedback\n",
        "\n",
        "## 1. Problem Statement\n",
        "\n",
        "Given an input input $x \\in \\mathcal{X}$, we want to predict a final output $y \\in \\Delta^K$ through a series of classification steps, with human feedback on the final output.\n",
        "\n",
        "## 2. Pipeline Definition\n",
        "\n",
        "Let the pipeline consist of $N$ classifiers !точно ли только классификаторы?!, denoted as $f_1, f_2, ..., f_N$. !запятые заменил на декартово произведение!\n",
        "\n",
        "- $f_1: \\mathcal{X} \\rightarrow \\mathcal{Y}_1$ (e.g., digit recognition)\n",
        "- $f_2: \\mathcal{X} \\times \\mathcal{Y}_1 \\rightarrow \\mathcal{Y}_2$\n",
        "- ...\n",
        "- $f_N: \\mathcal{X} \\times \\mathcal{Y}_1 \\times \\ldots \\times \\mathcal{Y}_{N-1} \\rightarrow \\Delta^K$\n",
        "\n",
        "Where $\\mathcal{Y}_i$ is the output space of the $i$-th classifier.\n",
        "\n",
        "!предлагаю добавит что-то типа:!\n",
        "For convenience we will sometimes use the following:\n",
        "- $p_j: \\mathcal{X}\\times\\mathcal{Y_1}\\times\\ldots\\times\\mathcal{Y_{j-1}} \\rightarrow \\mathcal{X}\\times\\mathcal{Y_1}\\times\\ldots\\times\\mathcal{Y_{j}}$\n",
        "with $p_j\\left(x, y_1, \\ldots y_{j-1}\\right) = \\left(x, y_1, \\ldots, y_{j-1}, f_j\\left(x, y_1, \\ldots, y_{j-1}\\right)\\right)$\n",
        "\n",
        "## 3. Forward Pass\n",
        "\n",
        "For an input $x$:\n",
        "\n",
        "$y_1 = f_1(x)$\n",
        "\n",
        "$y_2 = f_2(x, y_1)$\n",
        "\n",
        "$\\dots$\n",
        "\n",
        "$y_N = f_N(x, y_1, \\ldots y_{N-1}) \\in \\Delta^K$\n",
        "\n",
        "## 4. Human Feedback\n",
        "\n",
        "Let $h: \\Delta^K \\rightarrow \\{0, 1\\}$ !добавил ${}^K$! represent human feedback, where:\n",
        "\n",
        "$h(y_N) = \\begin{cases}\n",
        "1 & \\text{if human confirms } y_N \\text{ is correct} \\\\\n",
        "0 & \\text{if human denies } y_N \\text{ is correct}\n",
        "\\end{cases}$\n",
        "\n",
        "## 5. Error Attribution\n",
        "\n",
        "Let $e_i$ be the probability that classifier $f_i$ made an error:\n",
        "\n",
        "$e_i = P(\\text{error in } f_i | h(y_N) = 0)$\n",
        "\n",
        "We can estimate $e_i$ based on classifier confidence or historical performance.\n",
        "Note, we consider that error in classifier could not fix errors in previous classifiers. Such cases could be addressed further.\n",
        "\n",
        "## 6. Learning Objective\n",
        "\n",
        "Our goal is to minimize the error of the entire pipeline:\n",
        "\n",
        "$\\min_{\\theta_1, ..., \\theta_N} \\mathbb{E}_{x \\sim \\mathcal{X}}[L(f_N(f_{N-1}(...f_1(x)...)), h(y_N))]$\n",
        "\n",
        "!эта композиция корректна только с заменой $f_j$ на $p_j$ ($j < N$)\n",
        "и её мб проще записать как $(f_N \\circ p_{N-1} \\circ \\ldots \\circ p_1)(x)$!\n",
        "\n",
        "Where $\\theta_i$ are the parameters of classifier $f_i$, and $L$ is a suitable loss function (e.g., binary cross-entropy).\n",
        "\n",
        "## 7. White box classifiers with differentiable functions\n",
        "\n",
        "### 7.1 Parameter Updates\n",
        "If we can compute gradients, we update the parameters of each classifier, when $h(y_N) = 0$:\n",
        "\n",
        "$\\theta_i \\leftarrow \\theta_i - \\alpha \\cdot e_i \\cdot \\nabla_{\\theta_i} L_i$\n",
        "\n",
        "Where $\\alpha$ is the learning rate, and $L_i$ is the loss specific to classifier $f_i$.\n",
        "\n",
        "### 7.2 End-to-End Variant\n",
        "\n",
        "In this case, we can formulate this as an end-to-end learning problem:\n",
        "\n",
        "$f: \\mathcal{X} \\rightarrow \\{0, 1\\}$\n",
        "\n",
        "With the learning objective:\n",
        "\n",
        "$\\min_{\\theta} \\mathbb{E}_{x \\sim \\mathcal{X}}[L(f(x), h(f(x)))]$\n",
        "\n",
        "Where $\\theta$ are the parameters of the end-to-end model $f$.\n",
        "\n",
        "## 8. Parameter Updates for Black-box Classifiers\n",
        "\n",
        "### 8.1 Parameter Updates\n",
        "When dealing with black-box classifiers, we can't compute gradients directly. Instead, we can use gradient-free optimization methods. Here are three approaches:\n",
        "\n",
        "### a) Bayesian Optimization\n",
        "\n",
        "We can model the performance of each classifier $f_i$ as a function of its parameters $\\theta_i$:\n",
        "\n",
        "$p_i(\\theta_i) = P(f_i \\text{ is correct} | \\theta_i)$\n",
        "\n",
        "We then use Bayesian optimization to find the optimal parameters:\n",
        "\n",
        "$\\theta_i^* = \\arg\\max_{\\theta_i} \\mathbb{E}[p_i(\\theta_i)]$\n",
        "\n",
        "This involves:\n",
        "\n",
        "1.  Building a probabilistic model (e.g., Gaussian Process) of $p_i(\\theta_i)$\n",
        "2.  Using an acquisition function to decide which $\\theta_i$ to try next\n",
        "3.  Updating the model based on observed performance\n",
        "\n",
        "### b) Evolutionary Strategies\n",
        "\n",
        "We can use evolutionary algorithms to optimize the parameters:\n",
        "\n",
        "1.  Initialize a population of parameter sets: ${\\theta_i^1, \\theta_i^2, ..., \\theta_i^M}$\n",
        "2.  Evaluate the fitness of each set based on classifier performance\n",
        "3.  Select the best-performing sets\n",
        "4.  Generate new parameter sets through crossover and mutation\n",
        "5.  Repeat steps 2-4 for multiple generations\n",
        "\n",
        "The fitness function could be:\n",
        "\n",
        "$F(\\theta_i) = \\sum_{x \\in \\mathcal{X}} h(y_N) \\cdot (1 - e_i)$\n",
        "\n",
        "Where $h(y_N)$ is the human feedback and $e_i$ is the error attribution for classifier $i$.\n",
        "\n",
        "### c. Online Learning for Black-box Classifiers\n",
        "\n",
        "For online learning scenarios, we can use multi-armed bandit algorithms:\n",
        "\n",
        "1.  Treat each classifier as having multiple \"arms\" (parameter configurations)\n",
        "2.  Use algorithms like Upper Confidence Bound (UCB) or Thompson Sampling to balance exploration and exploitation\n",
        "3.  Update the probability distribution over arms based on observed performance\n",
        "\n",
        "The reward for each arm could be:\n",
        "\n",
        "$r(\\theta_i) = h(y_N) \\cdot (1 - e_i)$\n",
        "\n",
        "### 8.2 End-to-End Black-box Optimization\n",
        "\n",
        "For the end-to-end variant with a black-box model:\n",
        "\n",
        "$f: \\mathcal{X} \\rightarrow {0, 1}$\n",
        "\n",
        "We can use similar techniques (Bayesian Optimization, Evolutionary Strategies, or Bandit Algorithms) to optimize the entire pipeline as a single black-box function.\n",
        "\n",
        "The objective function becomes:\n",
        "\n",
        "$J(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{X}}[h(f(x; \\theta))]$\n",
        "\n",
        "Where $\\theta$ are the parameters of the end-to-end model $f$.\n",
        "\n",
        "## 9. Pipeline with Fit/Predict Classifiers\n",
        "\n",
        "*Все, что ниже нужно править.*\n",
        "\n",
        "When classifiers only expose fit and predict functions, we need to adjust our approach. Let's define these functions for each classifier $f_i$:\n",
        "\n",
        "-   $fit_i(X_i, Y_i) \\rightarrow f_i$: Trains the classifier on input data $X_i$ and labels $Y_i$\n",
        "-   $predict_i(X_i) \\rightarrow \\hat{Y}_i$: Makes predictions on input data $X_i$\n",
        "\n",
        "### 9.1 Data Flow\n",
        "\n",
        "For a pipeline with N classifiers:\n",
        "\n",
        "!здесь та же неточность, что и в композиции выше!\n",
        "\n",
        "1.  $f_1: fit_1(X, Y_1), predict_1(X) \\rightarrow \\hat{Y}_1$\n",
        "2.  $f_2: fit_2(\\hat{Y}_1, Y_2), predict_2(\\hat{Y}_1) \\rightarrow \\hat{Y}_2$\n",
        "3.  ...\n",
        "4.  $f_N: fit_N(\\hat{Y}_{N-1}, Y_N), predict_N(\\hat{Y}_{N-1}) \\rightarrow \\hat{Y}_N$\n",
        "\n",
        "Where $X$ is the original input data, and $Y_i$ are the labels for each classifier.\n",
        "\n",
        "### 9.2 Training Process\n",
        "\n",
        "1.  Initial Training:\n",
        "    -   Train $f_1$ on the original data: $fit_1(X, Y_1)$\n",
        "    -   Generate predictions: $\\hat{Y}_1 = predict_1(X)$\n",
        "    -   Train $f_2$ on these predictions: $fit_2(\\hat{Y}_1, Y_2)$\n",
        "    -   Continue this process for all classifiers\n",
        "2.  Feedback Incorporation:\n",
        "    -   Collect human feedback $h(\\hat{Y}_N)$ on the final predictions\n",
        "    -   Create feedback-adjusted datasets for each classifier\n",
        "\n",
        "### 9.3 Feedback-Adjusted Training\n",
        "\n",
        "#### 9.3.1 Only positive examples\n",
        "For each classifier $f_i$, create a feedback-adjusted dataset:\n",
        "\n",
        "$X_i' = {\\hat{Y}_{i-1}[j] \\mid h(\\hat{Y}_N[j]) = 1}$\n",
        "$Y_i' = {Y_i[j] \\mid h(\\hat{Y}_N[j]) = 1}$\n",
        "\n",
        "Then retrain: $fit_i(X_i', Y_i')$\n",
        "\n",
        "#### 9.3.2 Incorporating negative examples\n",
        "\n",
        "Instead of using only positive examples, we'll use both positive and negative feedback to create a more balanced and informative training set. We'll also introduce a weighting scheme to control the influence of each type of feedback.\n",
        "\n",
        "\n",
        "For each classifier $f_i$, create a feedback-adjusted dataset:\n",
        "\n",
        "$X_i' = {\\hat{Y}_{i-1}[j] \\mid j \\in \\text{all examples}}$\n",
        "$Y_i' = {Y_i'[j] \\mid j \\in \\text{all examples}}$\n",
        "$W_i' = {w_i[j] \\mid j \\in \\text{all examples}}$\n",
        "\n",
        "Where $Y_i'[j]$ and $w_i[j]$ are defined as:\n",
        "\n",
        "$Y_i'[j] = \\begin{cases} Y_i[j] & \\text{if } h(\\hat{Y}_N[j]) = 1 \\text{ (positive feedback)} \\\\ \\text{uncertain} & \\text{if } h(\\hat{Y}_N[j]) = 0 \\text{ (negative feedback)} \\end{cases}$\n",
        "\n",
        "$w_i[j] = \\begin{cases} 1 + \\alpha & \\text{if } h(\\hat{Y}_N[j]) = 1 \\text{ (positive feedback)} \\\\ \\beta & \\text{if } h(\\hat{Y}_N[j]) = 0 \\text{ (negative feedback)} \\end{cases}$\n",
        "\n",
        "Here, $\\alpha$ and $\\beta$ are hyperparameters that control the emphasis on positive and negative feedback respectively, where $0 \\leq \\alpha, \\beta \\leq 1$.\n",
        "\n",
        "#### 9.3.1 Handling Negative Feedback\n",
        "\n",
        "!что-то с нумерацией -- здесь 9.3.2.1 или 9.3.3?!\n",
        "\n",
        "For examples with negative feedback, we have several options:\n",
        "\n",
        "1.  Uncertainty Labeling: Mark these examples as \"uncertain\" and use a loss function that doesn't penalize predictions for uncertain labels.\n",
        "2.  Label Flipping: If the task is binary classification, we can flip the label:\n",
        "$Y_i'[j] = 1 - Y_i[j]$ if $h(\\hat{Y}_N[j]) = 0$\n",
        "3.  Soft Labeling: Assign probabilistic labels based on the classifier's current predictions and the negative feedback: $Y_i'[j] = (1 - \\lambda) \\cdot Y_i[j] + \\lambda \\cdot (1 - Y_i[j])$ if $h(\\hat{Y}_N[j]) = 0$ Where $\\lambda$ is a hyperparameter controlling the strength of label adjustment.\n",
        "4.  Exclusion: In some cases, it might be best to exclude negative feedback examples from training certain classifiers if we can't determine the correct label.\n",
        "\n",
        "#### 9.3.2 Training Process\n",
        "\n",
        "The training process now depends on how we handle negative feedback:\n",
        "\n",
        "1.  For Uncertainty Labeling: Implement a custom loss function that ignores uncertain labels.\n",
        "2.  For Label Flipping or Soft Labeling: $fit_i(X_i', Y_i', sample\\_weight=W_i')$\n",
        "3.  For Exclusion: $fit_i(X_i'positive, Y_i'positive, sample_weight=W_i'positive)$\n",
        "\n",
        "#### 9.3.3 Error Attribution in Multi-Step Pipelines\n",
        "\n",
        "For multi-step pipelines, we still need to attribute errors to specific classifiers:\n",
        "\n",
        "$e_i[j] = P(\\text{error in } f_i \\mid h(\\hat{Y}_N[j]) = 0)$\n",
        "\n",
        "Adjust the weights and handling of negative feedback based on these error attributions:\n",
        "\n",
        "$w_i[j] = \\begin{cases} 1 + \\alpha & \\text{if } h(\\hat{Y}_N[j]) = 1 \\\\ \\beta \\cdot e_i[j] & \\text{if } h(\\hat{Y}_N[j]) = 0 \\end{cases}$\n",
        "\n",
        "This approach concentrates the impact of negative feedback on the classifiers most likely to have caused the error.\n",
        "\n",
        "### 9.4 Inferring Positive Examples from Negative Feedback using EM Algorithm\n",
        "\n",
        "We can use the Expectation-Maximization (EM) algorithm to infer likely positive examples from instances where we received negative feedback. This approach is particularly useful when we have a multi-class problem and negative feedback only tells us that the predicted class was incorrect, but doesn't specify which class would have been correct.\n",
        "\n",
        "#### 9.4.1 EM Algorithm for Feedback Incorporation\n",
        "\n",
        "Let's define our problem:\n",
        "\n",
        "-   $X = {x_1, ..., x_N}$: Our set of examples\n",
        "-   $Y = {y_1, ..., y_K}$: The set of possible classes\n",
        "-   $F = {f_1, ..., f_N}$: Feedback for each example (1 for positive, 0 for negative)\n",
        "-   $\\theta$: Parameters of our classifier\n",
        "\n",
        "The EM algorithm proceeds as follows:\n",
        "\n",
        "1.  Initialization:\n",
        "    -   Initialize $\\theta$ using the current classifier parameters\n",
        "2.  E-step: For each example $x_i$ with negative feedback ($f_i = 0$): Calculate $P(y_k | x_i, \\theta)$ for all classes $k$ except the predicted class.\n",
        "3.  M-step: Update $\\theta$ to maximize the expected log-likelihood: $\\theta = \\arg\\max_\\theta \\sum_{i: f_i=0} \\sum_{k \\neq predicted} P(y_k | x_i, \\theta) \\log P(x_i, y_k | \\theta)$\n",
        "4.  Repeat steps 2-3 until convergence or for a fixed number of iterations.\n",
        "\n",
        "#### 9.4.2 Incorporating EM Results into Training\n",
        "\n",
        "After running the EM algorithm:\n",
        "\n",
        "1.  For positive feedback examples: Use the original labels and weights as before.\n",
        "2.  For negative feedback examples:\n",
        "    -   Use the class probabilities inferred by EM as soft labels.\n",
        "    -   Adjust weights based on the confidence of these inferred labels.\n",
        "\n",
        "$X_i' = {\\hat{Y}_{i-1}[j] \\mid j \\in \\text{all examples}}$ $Y_i' = \\begin{cases} Y_i[j] & \\text{if } f_j = 1 \\ P(y | x_j, \\theta) & \\text{if } f_j = 0 \\end{cases}$\n",
        "\n",
        "$W_i' = \\begin{cases} 1 + \\alpha & \\text{if } f_j = 1 \\ \\beta \\cdot \\max_k P(y_k | x_j, \\theta) & \\text{if } f_j = 0 \\end{cases}$\n",
        "\n",
        "Where $P(y | x_j, \\theta)$ is the probability distribution over classes inferred by the EM algorithm.\n",
        "\n",
        "#### 9.4.3 Training Process\n",
        "\n",
        "Train the classifier using these EM-inferred soft labels and weights:\n",
        "\n",
        "$fit_i(X_i', Y_i', sample_weight=W_i')$\n",
        "\n",
        "Note: Ensure your classifier can handle soft labels (probability distributions) as targets. If not, you may need to implement a custom loss function.\n",
        "\n",
        "#### 9.4.4 Iterative Refinement\n",
        "\n",
        "This process can be done iteratively:\n",
        "\n",
        "1.  Run the EM algorithm to infer labels for negative feedback examples.\n",
        "2.  Train the classifier using both positive feedback and EM-inferred examples.\n",
        "3.  Use the updated classifier to get new predictions.\n",
        "4.  Repeat the process with new feedback.\n",
        "\n",
        "### 9.5 Online Learning\n",
        "\n",
        "For online learning, we can use a sliding window or incremental learning approach:\n",
        "\n",
        "1.  Maintain a buffer of recent data points and their human feedback\n",
        "2.  Periodically retrain classifiers on this buffer: $fit_i(X_i^{buffer}, Y_i^{buffer})$\n",
        "3.  For classifiers that support partial_fit: $partial\\_fit_i(X_i^{new}, Y_i^{new})$\n",
        "\n",
        "### 9.6 Optimization Strategies\n",
        "\n",
        "1.  Hyperparameter Tuning:\n",
        "    -   Use techniques like Random Search or Bayesian Optimization to tune hyperparameters of each $fit_i$ function\n",
        "    -   Objective: Maximize pipeline accuracy based on human feedback\n",
        "2.  Ensemble Methods:\n",
        "    -   Train multiple versions of each classifier with different hyperparameters\n",
        "    -   Combine their predictions (e.g., voting, averaging)\n",
        "3.  Data Augmentation:\n",
        "    -   Generate synthetic data points based on confident predictions and human feedback\n",
        "    -   Use these to augment training data for each classifier\n",
        "\n",
        "### 9.7 Performance Metric\n",
        "\n",
        "Define a performance metric based on human feedback:\n",
        "\n",
        "$Performance = \\frac{1}{|X|} \\sum_{x \\in X} h(predict_N(predict_{N-1}(...predict_1(x)...)))$\n",
        "\n",
        "Optimize this metric through iterative training and feedback incorporation.\n"
      ],
      "metadata": {
        "id": "yyMa5VA3jmt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = dict(\n",
        "    seed  = 12309,\n",
        "    batch = 128,\n",
        "    metrics = [\"accuracy\",],\n",
        "    drift = \"flip\",\n",
        "    alg = \"greedy\",\n",
        ")\n",
        "for p in parameters:\n",
        "    if p not in locals():\n",
        "        v = parameters[p]\n",
        "        if isinstance(v, str):\n",
        "            exec(f\"{p} = '{v}'\")\n",
        "        else:\n",
        "            exec(f\"{p} = {v}\")\n"
      ],
      "metadata": {
        "id": "5gUZQMp_P7QE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qorioEycH-oF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "tf.random.set_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def printeval(evvals, evkeys=[\"loss\"] + metrics):\n",
        "    for k, v in zip(evkeys, evvals):\n",
        "        print(f\"{k}:\\t{v:.4f}\")\n",
        "\n",
        "def flipaug(img, lbl):\n",
        "    return tf.image.random_flip_left_right(img), lbl\n",
        "\n",
        "\n",
        "def drifted(dataset, seed=seed, drift=drift):\n",
        "    drift = drift.lower()\n",
        "    if drift == \"flip\":\n",
        "        return dataset.map(flipaug)\n",
        "    else:\n",
        "        raise ValueError(f\"Drift {drift} is unknown\")\n"
      ],
      "metadata": {
        "id": "y1-z_Du2YLW9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = datasets.mnist\n",
        "\n",
        "Xy_train, Xy_val = mnist.load_data()\n",
        "def mapimg(img, label):\n",
        "    return tf.image.convert_image_dtype(img, dtype=tf.float32), label\n",
        "\n",
        "train_ref = (\n",
        "    tf.data.Dataset.from_tensor_slices(Xy_train)\n",
        "    .shuffle(Xy_train[0].shape[0])\n",
        "    .batch(batch)\n",
        "    .map(mapimg)\n",
        ")\n",
        "val_ref = (\n",
        "    tf.data.Dataset.from_tensor_slices(Xy_val)\n",
        "    .shuffle(Xy_val[0].shape[0])\n",
        "    .batch(batch)\n",
        "    .map(mapimg)\n",
        ")\n",
        "train_drifted = drifted(train_ref)\n",
        "val_drifted = drifted(val_ref)\n"
      ],
      "metadata": {
        "id": "-pi_Y9rlJURe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_loss_fn(\n",
        "    model,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    patience=None,\n",
        "    factoring_layer=\"factor\",\n",
        "):\n",
        "    factoring_layer = model.get_layer(factoring_layer)\n",
        "    def wrapped(trg, logit):\n",
        "        inferred_lbls, miss_mask, diff_mask = factoring_layer.greedy_lblprop(\n",
        "            tf.stop_gradient(logits), trg, patience=patience,\n",
        "        )\n",
        "        match_mask = ~miss_mask\n",
        "        return loss(\n",
        "            tf.boolean_mask(inferred_lbls, match_mask),\n",
        "            tf.boolean_mask(logits, match_mask),\n",
        "        )\n",
        "    return wrapped\n",
        "\n",
        "class Factor(layers.Layer):\n",
        "    def __init__(self, base=2, patience=1, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.base = tf.constant(base, dtype=tf.int32, shape=())\n",
        "        self.patience = patience\n",
        "\n",
        "    def call(self, logits):\n",
        "        return self.factor(tf.argmax(logits, axis=-1))\n",
        "\n",
        "    def factor(self, labels):\n",
        "        return tf.cast(tf.keras.ops.mod(labels, self.base), tf.int32)\n",
        "\n",
        "    def refactor(self, logits, miss_mask, ninf=-1.0e+06):\n",
        "        am = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        mask = am[:, tf.newaxis] == tf.range(logits.shape[-1], dtype=tf.int32)[tf.newaxis, :]\n",
        "        mask &= miss_mask[:, tf.newaxis]\n",
        "        logits = tf.where(mask, ninf, logits)\n",
        "        return self(logits), logits\n",
        "\n",
        "    def greedy_lblprop(self, logits, trg, patience=None):\n",
        "        # this line is entirely to satisfy tf static checker\n",
        "        trg = tf.cast(trg, tf.int32)\n",
        "        if patience is None:\n",
        "            patience = self.patience\n",
        "        assert patience >= 0, f\"for greedy label propagation specify patience >= 0, not {patience}\"\n",
        "        guess = self(logits)\n",
        "        first_miss_mask = guess != trg\n",
        "        miss_mask = first_miss_mask\n",
        "        # this implementation is suboptimal\n",
        "        # because it recomputes good values\n",
        "        for d in range(patience):\n",
        "            guess, logits = self.refactor(logits, miss_mask)\n",
        "            miss_mask = guess != trg\n",
        "        diff_mask = first_miss_mask ^ miss_mask\n",
        "        # we could smooth over miss_mask, not simply argmax\n",
        "        return tf.argmax(logits, axis=-1, output_type=tf.int32), miss_mask, diff_mask\n",
        "\n",
        "# input\n",
        "inp = tf.keras.Input((28, 28, 1), name=\"img\")\n",
        "# cnn\n",
        "cnn = tf.keras.Sequential([\n",
        "    layers.InputLayer((28, 28, 1), name=\"inp\"),\n",
        "    layers.Conv2D(2, (5, 5), activation='relu', name=\"conv1\"),\n",
        "    layers.MaxPooling2D((2, 2), name=\"pool1\"),\n",
        "    layers.Conv2D(4, (5, 5), activation='relu', name=\"conv2\"),\n",
        "    layers.MaxPooling2D((2, 2), name=\"pool2\"),\n",
        "    layers.Conv2D(10, (4, 4), activation='relu', name=\"conv3\"),\n",
        "    layers.Flatten(name=\"flatten\"),\n",
        "    layers.Dense(10, name=\"logit\"),\n",
        "], name=\"logit\")\n",
        "# logit = layers.Conv2D(2, (5, 5), activation='relu', name=\"conv1\")(inp)\n",
        "# logit = layers.MaxPooling2D((2, 2), name=\"pool1\")(logit)\n",
        "# logit = layers.Conv2D(4, (5, 5), activation='relu', name=\"conv2\")(logit)\n",
        "# logit = layers.MaxPooling2D((2, 2), name=\"pool2\")(logit)\n",
        "# logit = layers.Conv2D(10, (4, 4), activation='relu', name=\"conv3\")(logit)\n",
        "# logit = layers.Flatten(name=\"flatten\")(logit)\n",
        "# outputs\n",
        "logit = cnn(inp)\n",
        "factor = Factor(base=2, name=\"factor\")(logit)\n",
        "\n",
        "pp = tf.keras.Model(inp, [factor, logit], name=\"Pipeline\")\n",
        "pp.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "oK9qdgVSLLLK",
        "outputId": "5d7b19c7-cfda-4ea9-cc9b-770a69522962"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Pipeline\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Pipeline\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ img (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ logit (\u001b[38;5;33mSequential\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,016\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ factor (\u001b[38;5;33mFactor\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m)                      │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ img (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ logit (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,016</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ factor (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Factor</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,016\u001b[0m (3.97 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,016</span> (3.97 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,016\u001b[0m (3.97 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,016</span> (3.97 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def factor_labels(model, factoring_layer=\"factor\"):\n",
        "    factoring_layer = model.get_layer(factoring_layer)\n",
        "    # @tf.function\n",
        "    def wrapped(img, lbl):\n",
        "        return img, factoring_layer.factor(lbl)\n",
        "    return wrapped\n",
        "\n",
        "train_drifted_factored = train_drifted.map(factor_labels(pp))\n",
        "val_drifted_factored = val_drifted.map(factor_labels(pp))\n"
      ],
      "metadata": {
        "id": "1dK7zkxrJa6_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.get_layer(\"logit\").compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=metrics,\n",
        ")\n",
        "pp.get_layer(\"logit\").fit(\n",
        "    train_ref, epochs=1,\n",
        "    validation_data=val_ref,\n",
        ")\n",
        "printeval(pp.get_layer(\"logit\").evaluate(val_ref))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty-S48ohuumj",
        "outputId": "7faba200-c3e1-48c4-afd2-5033840b9cc9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - accuracy: 0.4775 - loss: 1.5542 - val_accuracy: 0.8780 - val_loss: 0.3982\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8742 - loss: 0.4048\n",
            "loss:\t0.3982\n",
            "accuracy:\t0.8780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "printeval(pp.get_layer(\"logit\").evaluate(val_drifted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu4Cew8dv3Zw",
        "outputId": "088cb10e-eeca-4d91-ba1e-b7574bbfb28e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5945 - loss: 2.0370\n",
            "loss:\t2.1133\n",
            "accuracy:\t0.5846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_train_loop(\n",
        "    pipeline,\n",
        "    train_dataset,\n",
        "    optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=\"accuracy\",\n",
        "    epochs=1,\n",
        "    val_dataset=None,\n",
        "    early_stopping_rounds=None,\n",
        "    use_tqdm=True,\n",
        "):\n",
        "    # use tf.keras.metrics.get(\"Accuracy\")\n",
        "    pass\n",
        "\n",
        "def pipeline_eval_loop(\n",
        "    pipeline,\n",
        "    val_dataset,\n",
        "    loss,\n",
        "    metrics,\n",
        "    use_tqdm=True,\n",
        "):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "v65Td8UtxyuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss=greedy_loss_fn(pp),\n",
        "    metrics=metrics,\n",
        ")\n",
        "\n",
        "# pp.fit(train_drifted_factored, epochs=1, validation_data=val_drifted_factored)\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (X_batch, y_batch) in enumerate(tqdm(train_drifted_factored)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            factors, logits = pp(X_batch, training=True)\n",
        "\n",
        "            loss_value = greedy_loss_fn(pp)(y_batch, logits)\n",
        "            for metric in pp.metrics:\n",
        "                if metric.name != \"loss\":\n",
        "                    metric.update_state(y_batch, factors)\n",
        "\n",
        "\n",
        "        grads = tape.gradient(loss_value, pp.trainable_weights)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, pp.trainable_weights))\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * batch))\n",
        "            for metric in pp.metrics:\n",
        "                if metric.name != \"loss\":\n",
        "                    m = metric.result()\n",
        "                    for mk, mv in m.items():\n",
        "                        print(f\"{mk}: {mv:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "2288749c1538480ebd6d11b12e92deab",
            "58b8b85dbbf94eff8f0df0d9f62f12dd",
            "4478eb5891054e43b00e398d2089d2d2",
            "9d3b2bfbd7cc43339f488162b2277b99",
            "9f2dade40a88419aacc2b4eb28510f88",
            "7784925b6d774ed7bf9623781bcc2395",
            "4226d4c8d65041428d93c51507a55d62",
            "1c9404adcd084c9fa07422c1bcb294e8",
            "867c63bed6eb44ebabb61693f8fd2a04",
            "bc96e6ed623f44909647837c7ea25546",
            "96ce55b0990e4ead9ffd154e2a331059"
          ]
        },
        "id": "ujiQXkycdRAN",
        "outputId": "af995bfd-5e8c-4a4d-f96f-ab437c3a6341"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/469 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2288749c1538480ebd6d11b12e92deab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 0.2363\n",
            "Seen so far: 128 samples\n",
            "accuracy: 0.5829\n",
            "factor_accuracy: 1.0000\n",
            "Training loss (for one batch) at step 200: 0.3857\n",
            "Seen so far: 25728 samples\n",
            "accuracy: 0.4407\n",
            "factor_accuracy: 0.6866\n",
            "Training loss (for one batch) at step 400: 0.3750\n",
            "Seen so far: 51328 samples\n",
            "accuracy: 0.3951\n",
            "factor_accuracy: 0.7232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logits = tf.convert_to_tensor([\n",
        "#     list(range(10)),  # odd, even\n",
        "#     [-i for i in range(10)],  # even, odd\n",
        "#     [0, 100, 0, 99, 0, 98, 0, 97, 0, 96],  # odd, odd\n",
        "#     [100, 0, 99, 0, 98, 0, 97, 0, 96, 0],  # even, even\n",
        "# ], dtype=tf.float32)\n",
        "# # test 1\n",
        "# expected_new_labels = tf.convert_to_tensor([8, 0, 3, 0], dtype=tf.int32)\n",
        "# expected_miss_mask = tf.convert_to_tensor([False, False, True, False])\n",
        "# expected_diff_mask = tf.convert_to_tensor([True, False, False, False])\n",
        "# actual_new_labels, actual_miss_mask, actual_diff_mask =greedy_lblprop(\n",
        "#     pp, logits, tf.convert_to_tensor([0, 0, 0, 0], dtype=tf.int32), _logits=True,\n",
        "# )\n",
        "# assert all(expected_new_labels == actual_new_labels)\n",
        "# assert all(expected_miss_mask == actual_miss_mask)\n",
        "# assert all(expected_diff_mask == actual_diff_mask)\n",
        "# # test 2\n",
        "# expected_new_labels = tf.convert_to_tensor([9, 1, 1, 2], dtype=tf.int32)\n",
        "# expected_miss_mask = tf.convert_to_tensor([False, False, False, True])\n",
        "# expected_diff_mask = tf.convert_to_tensor([False, True, False, False])\n",
        "# actual_new_labels, actual_miss_mask, actual_diff_mask =greedy_lblprop(\n",
        "#     pp, logits, tf.convert_to_tensor([1, 1, 1, 1], dtype=tf.int32), _logits=True,\n",
        "# )\n",
        "# assert all(expected_new_labels == actual_new_labels)\n",
        "# assert all(expected_miss_mask == actual_miss_mask)\n",
        "# assert all(expected_diff_mask == actual_diff_mask)\n"
      ],
      "metadata": {
        "id": "_lpVPqMvnOVD"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yB5Pf-lLwXnq"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xR4NcWDtRawP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}